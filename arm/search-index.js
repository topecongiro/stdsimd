var N = null;var searchIndex = {};
searchIndex["coresimd"]={"doc":"SIMD and vendor intrinsics support library.","items":[[0,"arch","coresimd","Platform dependent vendor intrinsics.",N,N],[0,"arm","coresimd::arch","Platform-specific intrinsics for the `arm` platform.",N,N],[3,"int8x4_t","coresimd::arch::arm","ARM-specific 32-bit wide vector of four packed `i8`.",N,N],[3,"uint8x4_t","","ARM-specific 32-bit wide vector of four packed `u8`.",N,N],[3,"int16x2_t","","ARM-specific 32-bit wide vector of two packed `i16`.",N,N],[3,"uint16x2_t","","ARM-specific 32-bit wide vector of two packed `u16`.",N,N],[5,"_rev_u16","","Reverse the order of the bytes.",N,[[["u16"]],["u16"]]],[5,"_rev_u32","","Reverse the order of the bytes.",N,[[["u32"]],["u32"]]],[5,"_rev_u16","","Reverse the order of the bytes.",N,[[["u16"]],["u16"]]],[5,"_rev_u32","","Reverse the order of the bytes.",N,[[["u32"]],["u32"]]],[5,"_clz_u8","","Count Leading Zeros.",N,[[["u8"]],["u8"]]],[5,"_clz_u16","","Count Leading Zeros.",N,[[["u16"]],["u16"]]],[5,"_clz_u32","","Count Leading Zeros.",N,[[["u32"]],["u32"]]],[5,"_rbit_u32","","Reverse the bit order.",N,[[["u32"]],["u32"]]],[5,"qadd","","Signed saturating addition",N,[[["i32"],["i32"]],["i32"]]],[5,"qsub","","Signed saturating subtraction",N,[[["i32"],["i32"]],["i32"]]],[5,"qadd8","","Saturating four 8-bit integer additions",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"qsub8","","Saturating two 8-bit integer subtraction",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"qsub16","","Saturating two 16-bit integer subtraction",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"qadd16","","Saturating two 16-bit integer additions",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"qasx","","Returns the 16-bit signed saturated equivalent of",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"qsax","","Returns the 16-bit signed saturated equivalent of",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"sadd16","","Returns the 16-bit signed saturated equivalent of",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"sadd8","","Returns the 8-bit signed saturated equivalent of",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"smlad","","Dual 16-bit Signed Multiply with Addition of products and 32-bit accumulation.",N,[[["int16x2_t"],["int16x2_t"],["i32"]],["i32"]]],[5,"smlsd","","Dual 16-bit Signed Multiply with Subtraction  of products and 32-bit accumulation and overflow detection.",N,[[["int16x2_t"],["int16x2_t"],["i32"]],["i32"]]],[5,"sasx","","Returns the 16-bit signed equivalent of",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"sel","","Select bytes from each operand according to APSR GE flags",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"shadd8","","Signed halving parallel byte-wise addition.",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"shadd16","","Signed halving parallel halfword-wise addition.",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"shsub8","","Signed halving parallel byte-wise subtraction.",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"shsub16","","Signed halving parallel halfword-wise subtraction.",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"smuad","","Signed Dual Multiply Add.",N,[[["int16x2_t"],["int16x2_t"]],["i32"]]],[5,"smuadx","","Signed Dual Multiply Add Reversed.",N,[[["int16x2_t"],["int16x2_t"]],["i32"]]],[5,"smusd","","Signed Dual Multiply Subtract.",N,[[["int16x2_t"],["int16x2_t"]],["i32"]]],[5,"smusdx","","Signed Dual Multiply Subtract Reversed.",N,[[["int16x2_t"],["int16x2_t"]],["i32"]]],[5,"usad8","","Sum of 8-bit absolute differences.",N,[[["int8x4_t"],["int8x4_t"]],["u32"]]],[5,"usad8a","","Sum of 8-bit absolute differences and constant.",N,[[["int8x4_t"],["int8x4_t"],["u32"]],["u32"]]],[11,"clone","","",0,[[["self"]],["int8x4_t"]]],[11,"fmt","","",0,[[["self"],["formatter"]],["result"]]],[11,"clone","","",1,[[["self"]],["uint8x4_t"]]],[11,"fmt","","",1,[[["self"],["formatter"]],["result"]]],[11,"clone","","",2,[[["self"]],["int16x2_t"]]],[11,"fmt","","",2,[[["self"],["formatter"]],["result"]]],[11,"clone","","",3,[[["self"]],["uint16x2_t"]]],[11,"fmt","","",3,[[["self"],["formatter"]],["result"]]]],"paths":[[3,"int8x4_t"],[3,"uint8x4_t"],[3,"int16x2_t"],[3,"uint16x2_t"]]};
searchIndex["stdsimd"]={"doc":"SIMD and vendor intrinsics support library.","items":[[0,"arch","stdsimd","SIMD and vendor intrinsics module.",N,N],[0,"arm","stdsimd::arch","Platform-specific intrinsics for the `arm` platform.",N,N],[5,"qsax","stdsimd::arch::arm","Returns the 16-bit signed saturated equivalent of",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"_rev_u32","","Reverse the order of the bytes.",N,[[["u32"]],["u32"]]],[5,"smusdx","","Signed Dual Multiply Subtract Reversed.",N,[[["int16x2_t"],["int16x2_t"]],["i32"]]],[3,"int8x4_t","","ARM-specific 32-bit wide vector of four packed `i8`.",N,N],[5,"qadd8","","Saturating four 8-bit integer additions",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"usad8","","Sum of 8-bit absolute differences.",N,[[["int8x4_t"],["int8x4_t"]],["u32"]]],[5,"shsub8","","Signed halving parallel byte-wise subtraction.",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[3,"uint8x4_t","","ARM-specific 32-bit wide vector of four packed `u8`.",N,N],[3,"int16x2_t","","ARM-specific 32-bit wide vector of two packed `i16`.",N,N],[5,"smlsd","","Dual 16-bit Signed Multiply with Subtraction  of products and 32-bit accumulation and overflow detection.",N,[[["int16x2_t"],["int16x2_t"],["i32"]],["i32"]]],[5,"_clz_u8","","Count Leading Zeros.",N,[[["u8"]],["u8"]]],[5,"qadd","","Signed saturating addition",N,[[["i32"],["i32"]],["i32"]]],[5,"smuad","","Signed Dual Multiply Add.",N,[[["int16x2_t"],["int16x2_t"]],["i32"]]],[5,"_clz_u32","","Count Leading Zeros.",N,[[["u32"]],["u32"]]],[5,"_rbit_u32","","Reverse the bit order.",N,[[["u32"]],["u32"]]],[5,"qsub","","Signed saturating subtraction",N,[[["i32"],["i32"]],["i32"]]],[5,"_rev_u16","","Reverse the order of the bytes.",N,[[["u16"]],["u16"]]],[5,"shadd16","","Signed halving parallel halfword-wise addition.",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"usad8a","","Sum of 8-bit absolute differences and constant.",N,[[["int8x4_t"],["int8x4_t"],["u32"]],["u32"]]],[3,"uint16x2_t","","ARM-specific 32-bit wide vector of two packed `u16`.",N,N],[5,"_clz_u16","","Count Leading Zeros.",N,[[["u16"]],["u16"]]],[5,"qsub16","","Saturating two 16-bit integer subtraction",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"sel","","Select bytes from each operand according to APSR GE flags",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"sadd16","","Returns the 16-bit signed saturated equivalent of",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"qasx","","Returns the 16-bit signed saturated equivalent of",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"shadd8","","Signed halving parallel byte-wise addition.",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"smlad","","Dual 16-bit Signed Multiply with Addition of products and 32-bit accumulation.",N,[[["int16x2_t"],["int16x2_t"],["i32"]],["i32"]]],[5,"qsub8","","Saturating two 8-bit integer subtraction",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[5,"shsub16","","Signed halving parallel halfword-wise subtraction.",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"sasx","","Returns the 16-bit signed equivalent of",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"qadd16","","Saturating two 16-bit integer additions",N,[[["int16x2_t"],["int16x2_t"]],["int16x2_t"]]],[5,"smuadx","","Signed Dual Multiply Add Reversed.",N,[[["int16x2_t"],["int16x2_t"]],["i32"]]],[5,"smusd","","Signed Dual Multiply Subtract.",N,[[["int16x2_t"],["int16x2_t"]],["i32"]]],[5,"sadd8","","Returns the 8-bit signed saturated equivalent of",N,[[["int8x4_t"],["int8x4_t"]],["int8x4_t"]]],[14,"is_x86_feature_detected","stdsimd","",N,N],[14,"is_aarch64_feature_detected","","",N,N],[14,"is_powerpc_feature_detected","","",N,N],[14,"is_powerpc64_feature_detected","","",N,N],[14,"is_mips_feature_detected","","",N,N],[14,"is_mips64_feature_detected","","",N,N],[14,"is_arm_feature_detected","","",N,N],[11,"try_from","stdsimd::arch::arm","",0,[[["u"]],["result"]]],[11,"from","","",0,[[["t"]],["t"]]],[11,"try_into","","",0,[[["self"]],["result"]]],[11,"into","","",0,[[["self"]],["u"]]],[11,"borrow","","",0,[[["self"]],["t"]]],[11,"borrow_mut","","",0,[[["self"]],["t"]]],[11,"get_type_id","","",0,[[["self"]],["typeid"]]],[11,"to_owned","","",0,[[["self"]],["t"]]],[11,"clone_into","","",0,N],[11,"clone","","",1,[[["self"]],["int16x2_t"]]],[11,"clone","","",2,[[["self"]],["uint16x2_t"]]],[11,"clone","","",3,[[["self"]],["uint8x4_t"]]],[11,"clone","","",0,[[["self"]],["int8x4_t"]]],[11,"fmt","","",1,[[["self"],["formatter"]],["result",["error"]]]],[11,"fmt","","",3,[[["self"],["formatter"]],["result",["error"]]]],[11,"fmt","","",0,[[["self"],["formatter"]],["result",["error"]]]],[11,"fmt","","",2,[[["self"],["formatter"]],["result",["error"]]]],[11,"try_from","","",3,[[["u"]],["result"]]],[11,"from","","",3,[[["t"]],["t"]]],[11,"try_into","","",3,[[["self"]],["result"]]],[11,"into","","",3,[[["self"]],["u"]]],[11,"borrow","","",3,[[["self"]],["t"]]],[11,"borrow_mut","","",3,[[["self"]],["t"]]],[11,"get_type_id","","",3,[[["self"]],["typeid"]]],[11,"to_owned","","",3,[[["self"]],["t"]]],[11,"clone_into","","",3,N],[11,"try_from","","",1,[[["u"]],["result"]]],[11,"from","","",1,[[["t"]],["t"]]],[11,"try_into","","",1,[[["self"]],["result"]]],[11,"into","","",1,[[["self"]],["u"]]],[11,"borrow","","",1,[[["self"]],["t"]]],[11,"borrow_mut","","",1,[[["self"]],["t"]]],[11,"get_type_id","","",1,[[["self"]],["typeid"]]],[11,"to_owned","","",1,[[["self"]],["t"]]],[11,"clone_into","","",1,N],[11,"try_from","","",2,[[["u"]],["result"]]],[11,"from","","",2,[[["t"]],["t"]]],[11,"try_into","","",2,[[["self"]],["result"]]],[11,"into","","",2,[[["self"]],["u"]]],[11,"borrow","","",2,[[["self"]],["t"]]],[11,"borrow_mut","","",2,[[["self"]],["t"]]],[11,"get_type_id","","",2,[[["self"]],["typeid"]]],[11,"to_owned","","",2,[[["self"]],["t"]]],[11,"clone_into","","",2,N]],"paths":[[3,"int8x4_t"],[3,"int16x2_t"],[3,"uint16x2_t"],[3,"uint8x4_t"]]};
initSearch(searchIndex);
